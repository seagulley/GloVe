{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seagulley/GloVe/blob/master/Understanding_chain_rule_annotated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "924abb8f",
      "metadata": {
        "id": "924abb8f"
      },
      "source": [
        "# Understanding the Chain Rule (with Code)\n",
        "\n",
        "Welcome! This notebook turns the **chain rule** into something you can see, code, and reason about.  \n",
        "We’ll move from **intuition → formal math → implementation → experiments & plots**.\n",
        "\n",
        "## Learning objectives\n",
        "By the end, you will be able to:\n",
        "- State and use the scalar **chain rule** for composite functions.\n",
        "- Interpret the **vector/Jacobian** form via computational graphs.\n",
        "- Implement a tiny gradient routine that applies the chain rule.\n",
        "- Verify gradients numerically and (optionally) with autodiff.\n",
        "- Read plots that illustrate gradient flow and sensitivity.\n",
        "\n",
        "> **How to use this notebook:** Each section begins with a short explanation.  \n",
        "> Some cells are marked **(Advanced)** and include more formal derivations. These are optional “extra‑mile” materials.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875577be",
      "metadata": {
        "id": "875577be"
      },
      "source": [
        "## Notebook Roadmap\n",
        "\n",
        "1. **Setup & helpers** — imports, utilities\n",
        "2. **Symbolic derivatives (optional)** — use algebra to check the chain rule\n",
        "3. **From scratch implementation** — tiny gradient code that applies the chain rule\n",
        "4. **Numeric checks** — finite differences to validate gradients\n",
        "5. **Autodiff sanity check (optional)** — compare against PyTorch autograd if present\n",
        "6. **Visualizations** — see how composition affects sensitivities\n",
        "7. **Mini‑exercises** — quick prompts to reinforce ideas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0fabe3",
      "metadata": {
        "id": "8a0fabe3"
      },
      "source": [
        "## The Chain Rule — Formal Statements (Advanced)\n",
        "\n",
        "**Scalar → scalar (single variable):** If $ y = f(g(x)) $ and both $f$ and $g$ are differentiable at the appropriate points, then\n",
        "$\n",
        "\\frac{dy}{dx} \\;=\\; f'(g(x)) \\cdot g'(x).\n",
        "$\n",
        "\n",
        "**Scalar → scalar (multi‑stage composition):** For $ y = f(g(h(x))) $,\n",
        "$\n",
        "\\frac{dy}{dx} \\;=\\; f'(g(h(x)))\\cdot g'(h(x))\\cdot h'(x).\n",
        "$\n",
        "\n",
        "**Vector form (Jacobian chain rule):** Let $ \\mathbf{u} = g(\\mathbf{x}) \\in \\mathbb{R}^m $, $ \\mathbf{y} = f(\\mathbf{u}) \\in \\mathbb{R}^p $. Then for differentiable $f,g$,\n",
        "$\n",
        "J_{f\\circ g}(\\mathbf{x}) \\;=\\; J_f(\\mathbf{u})\\; J_g(\\mathbf{x}) \\quad\\text{with}\\quad \\mathbf{u}=g(\\mathbf{x}).\n",
        "$\n",
        "Equivalently, for a scalar output $y$ and vector input $\\mathbf{x}$,\n",
        "$\n",
        "\\nabla_{\\mathbf{x}}\\, y \\;=\\; J_g(\\mathbf{x})^\\top\\, \\nabla_{\\mathbf{u}}\\, y.\n",
        "$\n",
        "\n",
        "**Computational graph view:** If a node $z$ depends on parents $\\{a_i\\}$, then a small change $d a_i$ induces\n",
        "$\n",
        "dz \\;=\\; \\sum_i \\frac{\\partial z}{\\partial a_i}\\, d a_i.\n",
        "$\n",
        "Backpropagation applies this repeatedly from outputs to inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2437cc",
      "metadata": {
        "id": "4b2437cc"
      },
      "source": [
        "# Multivariate function and its derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e73dac15",
      "metadata": {
        "id": "e73dac15"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "097b0be6",
      "metadata": {
        "id": "097b0be6"
      },
      "source": [
        "### Autodiff / Backprop Sanity Check (Optional)\n",
        "\n",
        "If PyTorch (or another autodiff library) is available, we check our results against `autograd`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e1d03a",
      "metadata": {
        "id": "11e1d03a"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b14476bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b14476bb",
        "outputId": "076c8ad6-3e70-422a-f681-74355c8f546c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector variable x: tensor([ 3., -1.,  0.,  1.])\n",
            "Function f at x: tensor(215.)\n",
            "Gradient of f at x: tensor([ 306., -144.,  -18., -310.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "\n",
        "print(\"Vector variable x:\",x.data)\n",
        "\n",
        "print(\"Function f at x:\",f.data)\n",
        "\n",
        "# compute gradient of f at x\n",
        "g = torch.autograd.grad(f,x)\n",
        "\n",
        "print(\"Gradient of f at x:\",g[0].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7edc1a",
      "metadata": {
        "id": "3c7edc1a",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Let's verify with math formula.\n",
        "\n",
        "$f(x_1,x_2,x_3,x_4) = (x_1+10x_2)^2 + 5(x_3-x_4)^2 + (x_2+2x_3)^4 + 10(x_1-x_4)^4$\n",
        "\n",
        "Let's evaluate these partial derivatives at\n",
        "\n",
        "$x_1 = 3, x_2 = -1, x_3 = 0, x_4 = 1$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_1} = 2(x_1+10x_2)+40(x_1-x_4)^3 = 2(3-10)+40(3-1)^3 = -14+320 = 306$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_2} = 20(x_1+10x_2)+4(x_2+2x_3)^3 = 20(3-10)+4(-1)^3 = -140-4 = -144$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_3} = 10(x_3-x_4)+8(x_2+2x_3)^3 = 10(-1)+8(-1)^3 = -10-8 = -18$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_4} = -10(x_3-x_4)-40(x_1-x_4)^3 = -10(-1)-40(3-2)^3 = 10-320 = -310$\n",
        "\n",
        "So, we have the gradient\n",
        "\n",
        "$\\nabla f(3,-1,0,1) = \\left[ 306,-144,-18,-310 \\right]$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea16c42",
      "metadata": {
        "id": "bea16c42"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaba7e2b",
      "metadata": {
        "id": "aaba7e2b"
      },
      "source": [
        "# Numerical Derivative\n",
        "### For any function we can also compute its numerical derivative, which is an approximation. The formula for computing numerical derivatives often uses Taylor series approximation from Calculus:\n",
        "$\\frac{\\partial f}{\\partial x_1} = \\frac{f(x_1+\\delta x_1,x_2,x_3,...)-f(x_1-\\delta x_1,x_2,x_3,...)}{2\\delta x_1},$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_2} = \\frac{f(x_1,x_2+\\delta x_2,x_3,...)-f(x_1,x_2-\\delta x_2,x_3,...)}{2\\delta x_2},$\n",
        "\n",
        "$...,$\n",
        "\n",
        "### and so on for each variable $x_i$. As a result, comoutation of numerical derivative is extremely inefficient for a function with a large number of variables. But it is often used as a check to see if your autograd code is working correctly, or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b07c082",
      "metadata": {
        "id": "1b07c082"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20380fd0",
      "metadata": {
        "id": "20380fd0"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2f5076",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a2f5076",
        "outputId": "a7f6b790-e598-40bb-fb68-c25be2cdd53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector variable x: tensor([ 3., -1.,  0.,  1.])\n",
            "Function f at x: tensor(215.)\n",
            "Exact gradient of f at x: tensor([ 306., -144.,  -18., -310.])\n",
            "Numerical gradient of f at x: tensor([ 305.9769, -143.9972,  -18.0054, -309.9976])\n"
          ]
        }
      ],
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "\n",
        "def compute_f(x):\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  return f\n",
        "\n",
        "f = compute_f(x)\n",
        "\n",
        "print(\"Vector variable x:\",x.data)\n",
        "\n",
        "print(\"Function f at x:\",f.data)\n",
        "\n",
        "# compute gradient of f at x\n",
        "g = torch.autograd.grad(f,x)\n",
        "\n",
        "print(\"Exact gradient of f at x:\",g[0].data)\n",
        "\n",
        "num_g = torch.zeros(4)\n",
        "h=1e-3\n",
        "eye = torch.eye(4) # 4-by-4 identity matrix\n",
        "for i in range(4):\n",
        "  num_g[i] = compute_f(x+h*eye[i,:]) - compute_f(x-h*eye[i,:])\n",
        "\n",
        "num_g = num_g/(2.0*h)\n",
        "\n",
        "print(\"Numerical gradient of f at x:\",num_g.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4ae59b",
      "metadata": {
        "id": "ba4ae59b"
      },
      "source": [
        "# Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2872ef54",
      "metadata": {
        "id": "2872ef54"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad030212",
      "metadata": {
        "id": "ad030212"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ccc70364",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccc70364",
        "outputId": "60ac478c-1ed6-44b8-f1c1-7e2b4bf03c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4])\n",
            "x: tensor([ 3., -1.,  0.,  1.], requires_grad=True)\n",
            "g: tensor([ 306., -144.,  -18., -310.])\n",
            "Current variable: [ 2.694 -0.856  0.018  1.31 ] Current function value: 215.0\n",
            "torch.Size([4])\n",
            "x: tensor([ 2.6940, -0.8560,  0.0180,  1.3100], grad_fn=<SubBackward0>)\n",
            "g: tensor([  94.3077, -119.5255,  -17.3309,  -93.1197])\n",
            "torch.Size([4])\n",
            "x: tensor([ 2.5997, -0.7365,  0.0353,  1.4031], grad_fn=<SubBackward0>)\n",
            "g: tensor([ 58.9994, -96.4817, -16.0392, -54.8516])\n",
            "torch.Size([4])\n",
            "x: tensor([ 2.5407, -0.6400,  0.0514,  1.4580], grad_fn=<SubBackward0>)\n",
            "g: tensor([ 43.0520, -77.8050, -15.3066, -36.7044])\n",
            "torch.Size([4])\n",
            "x: tensor([ 2.4976, -0.5622,  0.0667,  1.4947], grad_fn=<SubBackward0>)\n",
            "g: tensor([ 34.1085, -62.8002, -14.9109, -26.0769])\n"
          ]
        }
      ],
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "\n",
        "steplength = 1e-3 # for gradient descent\n",
        "for i in range(5):\n",
        "  # function\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  # compute grdaient\n",
        "  g = torch.autograd.grad(f,x)\n",
        "  print(g[0].shape)\n",
        "  print(\"x:\", x)\n",
        "  print(\"g:\", g[0])\n",
        "  # adjust variable\n",
        "  x = x - steplength*g[0]\n",
        "  if i%100==0:\n",
        "    print(\"Current variable:\",x.detach().numpy(),\"Current function value:\", f.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b27f613c",
      "metadata": {
        "id": "b27f613c"
      },
      "source": [
        "# Gradient descent using PyTorch's optmization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca3fe26",
      "metadata": {
        "id": "6ca3fe26"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3caae38c",
      "metadata": {
        "id": "3caae38c"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8859700f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "8859700f",
        "outputId": "78a38be2-55f0-4e8d-bccf-92a0f1536e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current variable: [ 2.694 -0.856  0.018  1.31 ] Current function value: 215.0\n",
            "Current variable: [ 1.549712   -0.2964205   0.73650396  1.6820586 ] Current function value: 11.441824913024902\n",
            "Current variable: [ 1.408362   -0.00583249  0.57971793  1.0532441 ] Current function value: 6.6437225341796875\n",
            "Current variable: [ 0.6763986  -0.03913701  0.24533094  1.1681743 ] Current function value: 5.709615707397461\n",
            "Current variable: [ 0.5996511  -0.13040473  0.5036107   0.53555894] Current function value: 1.093517541885376\n",
            "Current variable: [ 0.59016085 -0.07009609  0.31826916  0.27739534] Current function value: 0.27947038412094116\n",
            "Current variable: [ 0.5269876  -0.03624894  0.12591662  0.26216415] Current function value: 0.16900770366191864\n",
            "Current variable: [ 0.45544916 -0.04479674  0.13050896  0.22580846] Current function value: 0.08700942993164062\n",
            "Current variable: [ 0.399531   -0.04679754  0.18029977  0.18939461] Current function value: 0.034884385764598846\n",
            "Current variable: [ 0.35905084 -0.03668797  0.18895635  0.19141704] Current function value: 0.022943012416362762\n"
          ]
        }
      ],
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-3, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "for i in range(100):\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  optimizer.zero_grad()\n",
        "  f.backward()\n",
        "  optimizer.step()\n",
        "  if i%10==0:\n",
        "    print(\"Current variable:\",x.detach().numpy(),\"Current function value:\", f.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c053e55",
      "metadata": {
        "id": "0c053e55"
      },
      "source": [
        "# Chain rule of derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8becbac6",
      "metadata": {
        "id": "8becbac6"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16220181",
      "metadata": {
        "id": "16220181"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c478c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2c478c2",
        "outputId": "23ead8c8-6ab5-4553-e0d6-b38c47eacfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable z: tensor([ 1., -1.])\n",
            "function x: tensor([2., 1., 1., 0.])\n",
            "function f: 390.0\n",
            "\n",
            "Gradient of f with respect x: tensor([ 344.,  348.,  226., -330.])\n",
            "\n",
            "Gradient of f with respec to z by two-step calculation: tensor([  710., -1126.])\n",
            "Gradient of f with respec to z by one-step calculation: tensor([  710., -1126.])\n"
          ]
        }
      ],
      "source": [
        "z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n",
        "\n",
        "print(\"Variable z:\",z.data)\n",
        "\n",
        "def compute_x(z):\n",
        "  x = torch.zeros(4)\n",
        "  x[0] = z[0] - z[1]\n",
        "  x[1] = z[0]**2\n",
        "  x[2] = z[1]**2\n",
        "  x[3] = z[0]**2+z[0]*z[1]\n",
        "  return x\n",
        "\n",
        "x = compute_x(z)\n",
        "print(\"function x:\",x.data)\n",
        "\n",
        "def compute_f(x):\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  return f\n",
        "\n",
        "f = compute_f(x)\n",
        "print(\"function f:\",f.item())\n",
        "print(\"\")\n",
        "#\n",
        "# Let's compute gradient of f with respect to x\n",
        "g_x = torch.autograd.grad(f,x,retain_graph=True,create_graph=True)\n",
        "print(\"Gradient of f with respect x:\",g_x[0].data)\n",
        "print(\"\")\n",
        "# Now compute Jacobian of x with respect to z and multiply with g_x to use chain rule\n",
        "g_z = torch.autograd.grad(x,z,g_x,retain_graph=True)\n",
        "\n",
        "# But PyTorch can compute derivative of f with respect to z directly - this is the amazing capability!\n",
        "g = torch.autograd.grad(f,z)\n",
        "\n",
        "print(\"Gradient of f with respec to z by two-step calculation:\",g_z[0].data)\n",
        "print(\"Gradient of f with respec to z by one-step calculation:\",g[0].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23819e5b",
      "metadata": {
        "id": "23819e5b"
      },
      "source": [
        "## Let's verify the result using math formula\n",
        "\n",
        "$z_1 = 1, z_2 = -1$\n",
        "\n",
        "$x_1 = z_1 - z_2 = 2, x_2 = z_1^2 = 1, x_3 = z_2^2 = 1, x_4 = z_1^2+z_1z_2 = 0$\n",
        "\n",
        "## Let's compute partial derivative of $f$ with respect to $x_i$:\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_1} = 2(x_1+10x_2)+40(x_1-x_4)^3 = 2(2+10)+40(2-0)^3 = 24+320 = 344$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_2} = 20(x_1+10x_2)+4(x_2+2x_3)^3 = 20(2+10)+4(1+2)^3 = 348$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_3} = 10(x_3-x_4)+8(x_2+2x_3)^3 = 10(1)+8(3)^3 = 10+216 = 226$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x_4} = -10(x_3-x_4)-40(x_1-x_4)^3 = -10(1)-40(2)^3 = -10-320 = -330$\n",
        "\n",
        "## Let's now compute partial derivative of $x_i$ with respect to $z_1$:\n",
        "\n",
        "$\\frac{\\partial x_1}{\\partial z_1} = 1$\n",
        "\n",
        "$\\frac{\\partial x_2}{\\partial z_1} = 2z_1 = 2(1) = 2$\n",
        "\n",
        "$\\frac{\\partial x_3}{\\partial z_1} = 0$\n",
        "\n",
        "$\\frac{\\partial x_4}{\\partial z_1} = 2z_1 + z_2 = 2(1)-1 = 1$\n",
        "\n",
        "## Let's also compute partial derivative of $x_i$ with respect to $z_2$:\n",
        "\n",
        "$\\frac{\\partial x_1}{\\partial z_2} = -1$\n",
        "\n",
        "$\\frac{\\partial x_2}{\\partial z_2} = 0$\n",
        "\n",
        "$\\frac{\\partial x_3}{\\partial z_2} = 2z_2 = -2$\n",
        "\n",
        "$\\frac{\\partial x_4}{\\partial z_2} = z_1 = 1$\n",
        "\n",
        "## Now we can apply the chain rule of derivative:\n",
        "\n",
        "$\\frac{\\partial f}{\\partial z_1} = \\sum_i \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial z_1} = 344(1) + 348(2) + 226(0) -330(1) = 710$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial z_2} = \\sum_i \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial z_2} = 344(-1) + 348(0) + 226(-2) -330(1) = -1126$\n",
        "\n",
        "## Using Jacobian notation, chain rule can be written as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_z f =\n",
        "    \\begin{bmatrix}\n",
        "        \\frac{\\partial f}{\\partial z_1}\\\\\n",
        "        \\frac{\\partial f}{\\partial z_2}\n",
        "    \\end{bmatrix} =\n",
        "    (J_z^x) (\\nabla_x f) =\n",
        "    \\begin{bmatrix}\n",
        "        \\frac{\\partial x_1}{\\partial z_1} & \\frac{\\partial x_2}{\\partial z_1} & \\frac{\\partial x_3}{\\partial z_1} & \\frac{\\partial x_4}{\\partial z_1}\\\\\n",
        "        \\frac{\\partial x_1}{\\partial z_2} & \\frac{\\partial x_2}{\\partial z_2} & \\frac{\\partial x_3}{\\partial z_2} & \\frac{\\partial x_4}{\\partial z_2}\n",
        "    \\end{bmatrix}\n",
        "    \\begin{bmatrix}\n",
        "        \\frac{\\partial f}{\\partial x_1}\\\\\n",
        "        \\frac{\\partial f}{\\partial x_2}\\\\\n",
        "        \\frac{\\partial f}{\\partial x_3}\\\\\n",
        "        \\frac{\\partial f}{\\partial x_4}\n",
        "    \\end{bmatrix} =\n",
        "    \\begin{bmatrix}\n",
        "        1 & 2 & 0 & 1\\\\\n",
        "        -1 & 0 & -2 & 1\n",
        "    \\end{bmatrix}    \n",
        "    \\begin{bmatrix}\n",
        "        344\\\\\n",
        "        348\\\\\n",
        "        226\\\\\n",
        "        -330\n",
        "    \\end{bmatrix} =\n",
        "    \\begin{bmatrix}\n",
        "        710\\\\\n",
        "        -1126\n",
        "    \\end{bmatrix}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723fd72c",
      "metadata": {
        "id": "723fd72c"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af90d7f8",
      "metadata": {
        "id": "af90d7f8"
      },
      "source": [
        "# Optimization by gradient descent: Assume $f$ is a loss function\n",
        "## We are minimizing $f$ by adjusting $z_1$ and $z_2$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e52dac",
      "metadata": {
        "id": "51e52dac"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84de5f90",
      "metadata": {
        "id": "84de5f90"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490e52e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "490e52e0",
        "outputId": "19f77b06-81ee-4a60-bc0b-bd7025eb594e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current variable (z) value: [0.28999996 0.12600005] Current function value: 390.0\n",
            "Current variable (z) value: [0.10041686 0.16472499] Current function value: 0.002078988589346409\n",
            "Current variable (z) value: [0.0918402  0.16432461] Current function value: 0.0010676763486117125\n",
            "Current variable (z) value: [0.08976527 0.1616178 ] Current function value: 0.0009482738096266985\n",
            "Current variable (z) value: [0.08847897 0.15887022] Current function value: 0.0008560288697481155\n",
            "Current variable (z) value: [0.08736441 0.15630378] Current function value: 0.0007776079582981765\n",
            "Current variable (z) value: [0.08633856 0.15392138] Current function value: 0.000710221123881638\n",
            "Current variable (z) value: [0.08538311 0.15170398] Current function value: 0.0006518377340398729\n",
            "Current variable (z) value: [0.08448897 0.14963281] Current function value: 0.0006008768104948103\n",
            "Current variable (z) value: [0.08364932 0.14769198] Current function value: 0.000556100916583091\n"
          ]
        }
      ],
      "source": [
        "steplength = 1e-3 # for gradient descent\n",
        "for i in range(1000):\n",
        "  # function\n",
        "  f = compute_f(compute_x(z))\n",
        "  # Compute gradient of f with respect to z directly\n",
        "  # PyTorch takes care of chain rule of derivatives\n",
        "  g = torch.autograd.grad(f,z)\n",
        "  # adjust variable\n",
        "  z = z - steplength*g[0]\n",
        "  if i%100==0:\n",
        "    print(\"Current variable (z) value:\",z.detach().numpy(),\"Current function value:\", f.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2f2aaf",
      "metadata": {
        "id": "be2f2aaf"
      },
      "source": [
        "# And of course optimization using PyTorch's gradient descent optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d904492",
      "metadata": {
        "id": "1d904492"
      },
      "source": [
        "_(Annotated)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a985d57",
      "metadata": {
        "id": "2a985d57"
      },
      "source": [
        "> **What happens here?** We build tensors that require gradients and let the library compute `dy/dx` automatically for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa470b3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "fa470b3d",
        "outputId": "bbc85f41-7b6e-410c-be80-ada8ad5a6159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current variable (z) value: [ 0.929      -0.88740003] Current function value: 390.0\n",
            "Current variable (z) value: [-0.19928932  0.4960087 ] Current function value: 0.9813486337661743\n",
            "Current variable (z) value: [-0.44388917  0.7940718 ] Current function value: 24.753379821777344\n",
            "Current variable (z) value: [-0.15692     0.49109557] Current function value: 2.4734973907470703\n",
            "Current variable (z) value: [0.00094466 0.29169223] Current function value: 0.23995020985603333\n",
            "Current variable (z) value: [0.06613181 0.20713682] Current function value: 0.02455221861600876\n",
            "Current variable (z) value: [0.09276582 0.17456606] Current function value: 0.0017178322887048125\n",
            "Current variable (z) value: [0.10217317 0.16280487] Current function value: 0.0023682680912315845\n",
            "Current variable (z) value: [0.10384288 0.1590711 ] Current function value: 0.003257445292547345\n",
            "Current variable (z) value: [0.10240351 0.1583022 ] Current function value: 0.002950004767626524\n"
          ]
        }
      ],
      "source": [
        "z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([z], lr=1e-4, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "for i in range(100):\n",
        "  f = compute_f(compute_x(z))\n",
        "  optimizer.zero_grad()\n",
        "  f.backward()\n",
        "  optimizer.step()\n",
        "  if i%10==0:\n",
        "    print(\"Current variable (z) value:\",z.detach().numpy(),\"Current function value:\", f.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac0a938",
      "metadata": {
        "id": "3ac0a938"
      },
      "source": [
        "### Example / Exploration\n",
        "\n",
        "An exploratory cell that supports the narrative above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b1f584",
      "metadata": {
        "id": "05b1f584"
      },
      "source": [
        "> **What happens here?** Supporting computations for the surrounding explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21dfd252",
      "metadata": {
        "id": "21dfd252"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "cf8ea552",
      "metadata": {
        "id": "cf8ea552"
      },
      "source": [
        "## Mini‑Exercises\n",
        "\n",
        "1. Let $ y = \\sin(3x^2+1) $. Compute $dy/dx$ by hand and then verify numerically.\n",
        "2. For $ \\mathbf{x}\\in\\mathbb{R}^2 $, $ u = W\\mathbf{x} + \\mathbf{b} $ (affine), $ y = \\sigma(u_1 u_2) $. Derive $\n",
        "\\nabla_{\\mathbf{x}} y $ using Jacobians.\n",
        "3. Replace the inner function with a nonlinearity (e.g., $g(x)=\t\\tanh(x)$ and plot how the gradient $dy/dx$ changes across inputs.\n",
        "4. (Advanced) Implement reverse‑mode accumulation on a small computational graph and print local partials as you backpropagate.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}